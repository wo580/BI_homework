{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking1\t什么是反向传播中的链式法则\t\t\t\n",
    "简要说明反向传播中的链式法则（10points）\n",
    "\n",
    "反向计算每一层的参数的偏导数,将输出层的误差反向传播给隐藏层(误差从输出一层一层往前传播，不可以跳过某些中间步骤，在计算每一步的误差时，需要乘上上一步得到的误差（链式法则，层层相乘）):\n",
    "使用权重矩阵相乘的形式，求解隐藏层的误差\n",
    "输出层误差在转置权重矩阵的帮助下，传递到了隐藏层，用来更新与隐藏层相连的权重矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking2\t请列举几种常见的激活函数，激活函数有什么作用\t\t\t\n",
    "简要说明常用的激活函数及作用（10points）\n",
    "\n",
    "在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数,\n",
    "激活函数的特点是非线性，而数据的分布绝大多数是非线性的，这样可以强化网络的学习能力\n",
    "\n",
    "sigmoid激活函数：输入为连续实值，输出结果为0和1之间\n",
    "f（x）=1/（1+e**-x）\n",
    "tanh激活函数：y=tanh(x)是奇函数，即图像过原点并且穿越第一、第三象限的严格单调递增曲线,值域：(-1, 1)\n",
    "tanh（x）=(e**x - e**-x)/(e**x + e**-x)\n",
    "relu激活函数：\n",
    "relu（x）=x,x>0\n",
    "relu（x）=0,x<=0\n",
    "\n",
    "应用：\n",
    "sigmoid和tanh函数输出值在(0,1)和(-1,1)之间 => 适合处理概率值;会产生梯度消失 => 不适合深层网络训练\n",
    "relu的有效导数是常数1，解决了深层网络中出现的梯度消失问题 => 更适合深层网络训练\n",
    "作用：非线性化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking3\t利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？\t\n",
    "能简要说明loss不变的解决方案（10points）\n",
    "\n",
    "有可能是梯度消失，神经网络迭代更新时，有些权值不更新==>可以通过梯度的检验来验证模型当前所处的状态,改变激活函数，改变权值的初始化等；\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
