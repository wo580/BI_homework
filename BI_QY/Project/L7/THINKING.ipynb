{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking1\t逻辑回归的假设条件是怎样的？\t\t\t\n",
    "能简要说明逻辑回归的假设条件（10points）\n",
    "\n",
    "假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，将数据进行二分类\n",
    "\n",
    "逻辑回归的第一个基本假设是数据服从伯努利分布。贝努力分布为离散型概率分布，如果成功，随机变量取值为1；如果失败，随机变量取值为0。成功概率记为p，失败概率为 q=1-p;(典型例子:抛硬币,非正即反，概率和为1)\n",
    "\n",
    "第二个假设是正类的概率由sigmoid函数计算，即\n",
    "g(z)=1/(1+$e^{(-1)}$  作用在特向量x和参数θ的内积${θ^T}$x上的结果:\n",
    "p=1/(1+$e^{-{θ^T}x})$\n",
    "假设函数的输出值在0-1之间\n",
    "\n",
    "预测样本为正的概率:p(y=1|x;0)=p\n",
    "预测样本为负的概率:p(y=0|x;0)=1-p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking2\t逻辑回归的损失函数是怎样的？\t\t\t\n",
    "能简要说明逻辑回归的损失函数（10points）\n",
    "\n",
    "逻辑回归是在线性函数的基础上，经过激活函数后产生的0~1之间的概率值\n",
    "设x为特征向量，y为真实的标签。y^是预测值,得出：\n",
    "if y = 1: p(y|x)=$h_\\theta(x)$\n",
    "if y = 0: p(y|x)=1-$h_\\theta(x)$\n",
    "合并后:\n",
    "p(y|x)=$h_\\theta(x)$(1-$h_\\theta(x)$)$^{1-y}$\n",
    "\n",
    "等式左边的p(y|x)，可以解读为给定特征向量x，模型能正确预测出y的能力。这个值总是大于0小于1的。越接近1说明能力越强。\n",
    "其实这个式子被成为似然函数，意思是预测结果靠近或者接近真是值的程度。想要模型更好，这个似然函数的值就要越大。\n",
    "最大化似然函数也就是最小化损失函数;\n",
    "\n",
    "构造损失函数:\n",
    "\n",
    "if y = 1: $Cost(h_\\theta(x), y) = −log(1−h_\\theta(x))$\n",
    "\n",
    "if y = 0: $ Cost(h_\\theta(x), y) = −log(h_\\theta(x)) $\n",
    "\n",
    "逻辑回归的损失函数是它的极大似然函数: $ L\\theta = logL(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{i}log(h_{\\theta}(x^i)) + (1-y^{i})log(1-h_{\\theta}(x^i)) $ \n",
    "\n",
    "其中：$y^i$指样本i的真实label值，而$h_{\\theta}(x^i)$指预测为该label的概率；\n",
    "\n",
    "同样的，最大化似然函数也就是最小化代价函数，因此可以去掉负号，并除以一个常数m对代价函数进行适当的缩放;\n",
    "\n",
    "最常见的方法有梯度下降法，坐标轴下降法，等牛顿法等，最常用的是梯度下降来不断逼近最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking3\t逻辑回归如何进行分类？\t\t\t\n",
    "能简要说明逻辑回归是如何进行分类的（10points）\n",
    "\n",
    "设定一个阈值，判断正类概率是否大于该阈值，一般阈值是0.5，所以只用判断正类概率是否大于0.5即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking4\t为什么在训练中需要将高度相关的特征去掉？\t\t\t\n",
    "简要说明为什么要去掉相关度高的特征（10points）\n",
    "\n",
    "（1）去掉高度相关的特征会让模型的可解释性更好。\n",
    "（2）可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。\n",
    "其次是特征多了，本身就会增大训练的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
