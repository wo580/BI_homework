{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking1\t参数共享指的是什么？\t\t\t\n",
    "能简要说明他们的作用（10points）\n",
    "\n",
    "参数共享指共享权重，隐藏层的参数个数和隐藏层的神经元个数无关，只和滤波器的大小和滤波器种类的多少有关，在不同的位置使用的是同一个卷积核进行卷积运算；\n",
    "\n",
    "<!-- （提取不同的特征，需要多个滤波器。每种滤波器的参数不一样，表示它提出输入图像的不同特征。这样每种滤波器进行卷积图像就得到对图像的不同特征的反映，我们称之为特征图Feature Map，100种卷积核就有100个Feature Map。这100个Feature Map就组成了一层神经元） -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking2\t为什么会用到batch normalization ?\t\t\n",
    "能简要说明BN的作用（10points）\t\n",
    "\n",
    "网络训练过程中参数不断改变导致后续每一层输入的分布也发生变化，而学习的过程又要使每一层适应输入的分布，因此我们需要降低学习率，小心初始化，一般在训练网络的时会将输入减去均值，有些时候会对输入做白化等操作，目的是为了加快训练；\n",
    "BN 就是通过一定的规范化手段，把每层神经网络任意神经元的输入值的分布强行拉回到均值为0、方差为1的标准正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，从而避免梯度消失问题。这样输入的小变化就会导致损失函数较大的变化（使得梯度变大，避免梯度消失问题产生），同时也让收敛速度更快，加快训练速度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thinking3\t使用dropout可以解决什么问题？\t\t\n",
    "能简要解决的问题（10points）\t\n",
    "\n",
    "dropout可以防止神经网络过拟合，使得神经网络的泛化能力更强。\n",
    "dropout在前向传播的时候，让某个神经元的激活值以一定的概率p暂时停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
