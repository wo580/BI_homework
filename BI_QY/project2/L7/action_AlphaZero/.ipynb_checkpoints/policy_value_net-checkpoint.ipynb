{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "An implementation of the policyValueNet in Theano and Lasagne\n",
    "\n",
    "@author: Junxiao Song\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import pickle\n",
    "\n",
    "\n",
    "class PolicyValueNet():\n",
    "    \"\"\"policy-value network \"\"\"\n",
    "    def __init__(self, board_width, board_height, model_file=None):\n",
    "        self.board_width = board_width\n",
    "        self.board_height = board_height\n",
    "        self.learning_rate = T.scalar('learning_rate')\n",
    "        self.l2_const = 1e-4  # coef of l2 penalty\n",
    "        self.create_policy_value_net()\n",
    "        self._loss_train_op()\n",
    "        if model_file:\n",
    "            try:\n",
    "                net_params = pickle.load(open(model_file, 'rb'))\n",
    "            except:\n",
    "                # To support loading pretrained model in python3\n",
    "                net_params = pickle.load(open(model_file, 'rb'),\n",
    "                                         encoding='bytes')\n",
    "            lasagne.layers.set_all_param_values(\n",
    "                    [self.policy_net, self.value_net], net_params\n",
    "                    )\n",
    "\n",
    "    def create_policy_value_net(self):\n",
    "        \"\"\"create the policy value network \"\"\"\n",
    "        self.state_input = T.tensor4('state')\n",
    "        self.winner = T.vector('winner')\n",
    "        self.mcts_probs = T.matrix('mcts_probs')\n",
    "        network = lasagne.layers.InputLayer(\n",
    "                shape=(None, 4, self.board_width, self.board_height),\n",
    "                input_var=self.state_input\n",
    "                )\n",
    "        # conv layers\n",
    "        network = lasagne.layers.Conv2DLayer(\n",
    "                network, num_filters=32, filter_size=(3, 3), pad='same')\n",
    "        network = lasagne.layers.Conv2DLayer(\n",
    "                network, num_filters=64, filter_size=(3, 3), pad='same')\n",
    "        network = lasagne.layers.Conv2DLayer(\n",
    "                network, num_filters=128, filter_size=(3, 3), pad='same')\n",
    "        # action policy layers\n",
    "        policy_net = lasagne.layers.Conv2DLayer(\n",
    "                network, num_filters=4, filter_size=(1, 1))\n",
    "        self.policy_net = lasagne.layers.DenseLayer(\n",
    "                policy_net, num_units=self.board_width*self.board_height,\n",
    "                nonlinearity=lasagne.nonlinearities.softmax)\n",
    "        # state value layers\n",
    "        value_net = lasagne.layers.Conv2DLayer(\n",
    "                network, num_filters=2, filter_size=(1, 1))\n",
    "        value_net = lasagne.layers.DenseLayer(value_net, num_units=64)\n",
    "        self.value_net = lasagne.layers.DenseLayer(\n",
    "                value_net, num_units=1,\n",
    "                nonlinearity=lasagne.nonlinearities.tanh)\n",
    "        # get action probs and state score value\n",
    "        self.action_probs, self.value = lasagne.layers.get_output(\n",
    "                [self.policy_net, self.value_net])\n",
    "        self.policy_value = theano.function([self.state_input],\n",
    "                                            [self.action_probs, self.value],\n",
    "                                            allow_input_downcast=True)\n",
    "\n",
    "    def policy_value_fn(self, board):\n",
    "        \"\"\"\n",
    "        input: board\n",
    "        output: a list of (action, probability) tuples for each available\n",
    "            action and the score of the board state\n",
    "        \"\"\"\n",
    "        legal_positions = board.availables\n",
    "        current_state = board.current_state()\n",
    "        act_probs, value = self.policy_value(\n",
    "            current_state.reshape(-1, 4, self.board_width, self.board_height)\n",
    "            )\n",
    "        act_probs = zip(legal_positions, act_probs.flatten()[legal_positions])\n",
    "        return act_probs, value[0][0]\n",
    "\n",
    "    def _loss_train_op(self):\n",
    "        \"\"\"\n",
    "        Three loss terms：\n",
    "        loss = (z - v)^2 - pi^T * log(p) + c||theta||^2\n",
    "        \"\"\"\n",
    "        params = lasagne.layers.get_all_params(\n",
    "                [self.policy_net, self.value_net], trainable=True)\n",
    "        value_loss = lasagne.objectives.squared_error(\n",
    "                self.winner, self.value.flatten())\n",
    "        policy_loss = lasagne.objectives.categorical_crossentropy(\n",
    "                self.action_probs, self.mcts_probs)\n",
    "        l2_penalty = lasagne.regularization.apply_penalty(\n",
    "                params, lasagne.regularization.l2)\n",
    "        self.loss = self.l2_const*l2_penalty + lasagne.objectives.aggregate(\n",
    "                value_loss + policy_loss, mode='mean')\n",
    "        # policy entropy，for monitoring only\n",
    "        self.entropy = -T.mean(T.sum(\n",
    "                self.action_probs * T.log(self.action_probs + 1e-10), axis=1))\n",
    "        # get the train op\n",
    "        updates = lasagne.updates.adam(self.loss, params,\n",
    "                                       learning_rate=self.learning_rate)\n",
    "        self.train_step = theano.function(\n",
    "            [self.state_input, self.mcts_probs, self.winner, self.learning_rate],\n",
    "            [self.loss, self.entropy],\n",
    "            updates=updates,\n",
    "            allow_input_downcast=True\n",
    "            )\n",
    "\n",
    "    def get_policy_param(self):\n",
    "        net_params = lasagne.layers.get_all_param_values(\n",
    "                [self.policy_net, self.value_net])\n",
    "        return net_params\n",
    "\n",
    "    def save_model(self, model_file):\n",
    "        \"\"\" save model params to file \"\"\"\n",
    "        net_params = self.get_policy_param()  # get model params\n",
    "        pickle.dump(net_params, open(model_file, 'wb'), protocol=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
