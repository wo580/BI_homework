{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用PyTorch实现策略价值网络 PolicyValueNet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 设置学习率\n",
    "def set_learning_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "# 策略网络模型\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, board_width, board_height):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.board_width = board_width\n",
    "        self.board_height = board_height\n",
    "        # 通用层 common layers\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        # 行动策略层 action policy layers\n",
    "        self.act_conv1 = nn.Conv2d(128, 4, kernel_size=1)\n",
    "        self.act_fc1 = nn.Linear(4*board_width*board_height,\n",
    "                                 board_width*board_height)\n",
    "        # 状态值层 state value layers\n",
    "        self.val_conv1 = nn.Conv2d(128, 2, kernel_size=1)\n",
    "        self.val_fc1 = nn.Linear(2*board_width*board_height, 64)\n",
    "        self.val_fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    # 得到预测结果，返回行动可能性和状态值\n",
    "    def forward(self, state_input):\n",
    "        # 通用层 common layers\n",
    "        x = F.relu(self.conv1(state_input))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # 行动策略层 action policy layers\n",
    "        x_act = F.relu(self.act_conv1(x))\n",
    "        x_act = x_act.view(-1, 4*self.board_width*self.board_height)\n",
    "        x_act = F.log_softmax(self.act_fc1(x_act))\n",
    "        # 状态值层 state value layers\n",
    "        x_val = F.relu(self.val_conv1(x))\n",
    "        x_val = x_val.view(-1, 2*self.board_width*self.board_height)\n",
    "        x_val = F.relu(self.val_fc1(x_val))\n",
    "        x_val = F.tanh(self.val_fc2(x_val))\n",
    "        # 输出行动可能性 和 终局的预期状态值\n",
    "        return x_act, x_val\n",
    "\n",
    "# 策略价值网络(包括了两个端口:policy / value network)\n",
    "class PolicyValueNet():\n",
    "    def __init__(self, board_width, board_height,\n",
    "                 model_file=None, use_gpu=False):\n",
    "        self.use_gpu = use_gpu\n",
    "        self.board_width = board_width\n",
    "        self.board_height = board_height\n",
    "        self.l2_const = 1e-4  # L2正则项系数\n",
    "        # 设置策略网络参数\n",
    "        if self.use_gpu:\n",
    "            self.policy_value_net = Net(board_width, board_height).cuda()\n",
    "        else:\n",
    "            self.policy_value_net = Net(board_width, board_height)\n",
    "        self.optimizer = optim.Adam(self.policy_value_net.parameters(), weight_decay=self.l2_const)\n",
    "\n",
    "        if model_file:\n",
    "            net_params = torch.load(model_file)\n",
    "            self.policy_value_net.load_state_dict(net_params)\n",
    "\n",
    "    # 输入状态，得到行动的可能性和状态值，按照batch进行输入\n",
    "    def policy_value(self, state_batch):\n",
    "        if self.use_gpu:\n",
    "            state_batch = Variable(torch.FloatTensor(state_batch).cuda())\n",
    "            log_act_probs, value = self.policy_value_net(state_batch)\n",
    "            # 通过e的次幂，求得action probabilities\n",
    "            act_probs = np.exp(log_act_probs.data.cpu().numpy())\n",
    "            return act_probs, value.data.cpu().numpy()\n",
    "        else:\n",
    "            state_batch = Variable(torch.FloatTensor(state_batch))\n",
    "            log_act_probs, value = self.policy_value_net(state_batch)\n",
    "            act_probs = np.exp(log_act_probs.data.numpy())\n",
    "            return act_probs, value.data.numpy()\n",
    "\n",
    "    # 相比于policy_value多了一个action位置，输入棋盘，输出所有可能的(action, probability)，以及棋盘状态分数[-1,1]\n",
    "    def policy_value_fn(self, board):\n",
    "        # 得到所有可以下棋的位置\n",
    "        legal_positions = board.availables\n",
    "        # ascontiguousarray函数将一个内存不连续存储的数组转换为内存连续存储的数组，使得运行速度更快\n",
    "        current_state = np.ascontiguousarray(board.current_state().reshape(-1, 4, self.board_width, self.board_height))\n",
    "        if self.use_gpu:\n",
    "            # 前向传播，直接输入数据即可\n",
    "            log_act_probs, value = self.policy_value_net(Variable(torch.from_numpy(current_state)).cuda().float())\n",
    "            act_probs = np.exp(log_act_probs.data.cpu().numpy().flatten())\n",
    "        else:\n",
    "            log_act_probs, value = self.policy_value_net(Variable(torch.from_numpy(current_state)).float())\n",
    "            act_probs = np.exp(log_act_probs.data.numpy().flatten())\n",
    "        act_probs = zip(legal_positions, act_probs[legal_positions])\n",
    "        value = value.data[0][0]\n",
    "        # 返回 act_probs:所有可能的(action, probability)，value: 棋盘状态分数\n",
    "        return act_probs, value\n",
    "\n",
    "    # 训练一步\n",
    "    def train_step(self, state_batch, mcts_probs, winner_batch, lr):\n",
    "        # 包装变量\n",
    "        if self.use_gpu:\n",
    "            state_batch = Variable(torch.FloatTensor(state_batch).cuda())\n",
    "            mcts_probs = Variable(torch.FloatTensor(mcts_probs).cuda())\n",
    "            winner_batch = Variable(torch.FloatTensor(winner_batch).cuda())\n",
    "        else:\n",
    "            state_batch = Variable(torch.FloatTensor(state_batch))\n",
    "            mcts_probs = Variable(torch.FloatTensor(mcts_probs))\n",
    "            winner_batch = Variable(torch.FloatTensor(winner_batch))\n",
    "        # 清空模型中参数的梯度，即梯度置为0\n",
    "        self.optimizer.zero_grad()\n",
    "        # 设置学习率\n",
    "        set_learning_rate(self.optimizer, lr)\n",
    "        # 前向传播\n",
    "        log_act_probs, value = self.policy_value_net(state_batch)\n",
    "        # 定义 loss = (z - v)^2 - pi^T * log(p) + c||theta||^2\n",
    "        value_loss = F.mse_loss(value.view(-1), winner_batch)\n",
    "        policy_loss = -torch.mean(torch.sum(mcts_probs*log_act_probs, 1))\n",
    "        loss = value_loss + policy_loss\n",
    "        # 反向传播，优化参数\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # 计算Policy信息熵\n",
    "        entropy = -torch.mean(torch.sum(torch.exp(log_act_probs) * log_act_probs, 1))\n",
    "        # 返回loss和entropy\n",
    "        return loss.item(), entropy.item()\n",
    "\n",
    "    # 获得模型的参数，即state_dict\n",
    "    def get_policy_param(self):\n",
    "        net_params = self.policy_value_net.state_dict()\n",
    "        return net_params\n",
    "\n",
    "    # 保存模型文件\n",
    "    def save_model(self, model_file):\n",
    "        # 保存模型的参数\n",
    "        net_params = self.get_policy_param()\n",
    "        torch.save(net_params, model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
